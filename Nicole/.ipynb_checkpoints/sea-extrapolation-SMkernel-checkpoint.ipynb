{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import cf_units\n",
    "import datetime\n",
    "import zarr\n",
    "import dask\n",
    "\n",
    "import dask.array as dsa\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load data from xarray\n",
    "sea_level_picontrol = xr.open_zarr('/scratch/aeb783/data/project/sea_level_picontrol.zarr')\n",
    "rechunked = xr.open_zarr('/scratch/aeb783/data/project/sea_level_picontrol_rechunked.zarr')\n",
    "\n",
    "# Enter coordinates \n",
    "coords = (29, -90)  #NOLA\n",
    "\n",
    "# Grab the approximate chunk for provided coords\n",
    "area_approx = rechunked.sea_level.sel(xt_ocean=coords[1], yt_ocean=coords[0], method=\"nearest\")\n",
    "\n",
    "# Set up the DataArray with data as the sea level of the approximate area, and time as it's dimension\n",
    "coords_dict = {'time': sea_level_picontrol['time']}\n",
    "area_sea_level = xr.DataArray(area_approx, dims=('time'), coords=coords_dict)\n",
    "\n",
    "# Turn Data to tensors\n",
    "time_int = cf_units.date2num(area_sea_level.time,'days since 0185-01-01 12:00:00' , calendar='julian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-footwear",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_x_and_y(indexes, time, sea_level):\n",
    "    \"\"\"\n",
    "    Input: array of indexes\n",
    "    Output: Tensors for x and y (either train or test)\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = [] \n",
    "\n",
    "    for i in indexes:\n",
    "        x.append(time[i])\n",
    "        y.append(sea_level[i])\n",
    "        \n",
    "    x = (np.array(x)).astype(float)\n",
    "    y = (np.array(y)).astype(float)\n",
    "        \n",
    "    x = torch.FloatTensor(x).squeeze()\n",
    "    y = torch.FloatTensor(y).squeeze()\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def remove_gaps(indexes, data_size, gaps, gap_size):\n",
    "    gaps_arr = np.arange(data_size-gap_size, data_size).reshape(-1, 1)\n",
    "    for gap in gaps_arr:\n",
    "        i = np.arange(gap, gap+gap_size)\n",
    "        indexes = np.delete(indexes, i, 0)\n",
    "        \n",
    "    return indexes\n",
    "        \n",
    "def select_training_gaps(data_size, gaps, gap_size, time, sea_level):\n",
    "    \"\"\"\n",
    "    Input: sea_level data\n",
    "    Output: Tensors with x_train y_train, x_test, and y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly choose points for training, the rest for testing\n",
    "    indexes = list(range(0, data_size))\n",
    "    training_indexes = remove_gaps(indexes, data_size, gaps, gap_size)\n",
    "    testing_indexes = np.setdiff1d(indexes, training_indexes).reshape(-1, 1)\n",
    "    \n",
    "    x_train, y_train = set_x_and_y(training_indexes, time, sea_level)\n",
    "    x_test, y_test = set_x_and_y(testing_indexes, time, sea_level)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Change how many points we're using, how many gaps in the data, and the size of the gaps\n",
    "data_size = 1000\n",
    "gaps = 1\n",
    "gap_size = 250\n",
    "\n",
    "#Input: size of data, number of gaps, size of gaps, time data, sea level data\n",
    "x_train, y_train, x_test, y_test = select_training_gaps(data_size, gaps, gap_size, time_int, area_sea_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Our training and testing points\n",
    "train_plt = plt.scatter(x_train, y_train)\n",
    "test_plt = plt.scatter(x_test, y_test)\n",
    "plt.legend((train_plt, test_plt),\n",
    "           ('Train points', 'Test points'),\n",
    "           loc='lower left',\n",
    "           scatterpoints=1,\n",
    "           ncol=3,\n",
    "           fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean std of x and y variables\n",
    "xtrain_mean = x_train.mean()\n",
    "ytrain_mean = y_train.mean()\n",
    "\n",
    "xtrain_std = x_train.std()\n",
    "ytrain_std = y_train.std()\n",
    "\n",
    "# Normalize training data\n",
    "x_train = (x_train - x_train.mean()) / x_train.std()\n",
    "y_train = (y_train - y_train.mean()) / y_train.std()\n",
    "\n",
    "# Normalize testing data\n",
    "x_test = (x_test - xtrain_mean) / xtrain_std\n",
    "y_test = (y_test - ytrain_mean) / ytrain_std\n",
    "\n",
    "# Full prediction\n",
    "full_x = torch.cat((x_train, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Our training and testing points\n",
    "train_plt = plt.plot(x_train, y_train, label=\"Training\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralMixtureGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, x_train, y_train, likelihood):\n",
    "        super(SpectralMixtureGPModel, self).__init__(x_train, y_train, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=40)\n",
    "        self.covar_module.initialize_from_data(x_train, y_train)\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = SpectralMixtureGPModel(x_train, y_train, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-disorder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 200\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = -mll(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-handy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances)\n",
    "# See https://arxiv.org/abs/1803.06058\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    \n",
    "    # Make predictions\n",
    "    observed_pred = likelihood(model(x_test))\n",
    "    observed_pred_full = likelihood(model(full_x))\n",
    "    \n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(11, 7))\n",
    "    \n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    lower_full, upper_full = observed_pred_full.confidence_region()\n",
    "\n",
    "    # Plot training data as black x's\n",
    "    ax.plot(x_train.numpy(), y_train.numpy(), 'x', color='black', markersize=4)\n",
    "    \n",
    "    # Plot test data as red line\n",
    "    ax.plot(x_test.numpy(), y_test.numpy(), '--', color='red', lw=1.5)\n",
    "\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(full_x.numpy(), observed_pred_full.mean.numpy(), 'mediumblue', lw=1.5)\n",
    "\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(full_x.numpy() , lower_full.numpy(), upper_full.numpy(), facecolor='mediumblue',alpha=0.15)\n",
    "    \n",
    "    ax.legend(['Training Data', 'True Test Data', 'GP Prediction', 'Confidence'])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
